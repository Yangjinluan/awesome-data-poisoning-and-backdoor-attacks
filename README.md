# Awesome Data Poisoning and Backdoor Attacks

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

> Disclaimer: This repository may not include all relevant papers in this area. Use at your own discretion and please contribute any missing or overlooked papers via pull request.

A curated list of papers & resources linked to data poisoning, backdoor attacks and defenses against them.

## Surveys

+ Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses (TPAMI 2022) [[paper](https://ieeexplore.ieee.org/abstract/document/9743317)]
+ A Survey on Data Poisoning Attacks and Defenses (DSC 2022) [[paper](https://ieeexplore.ieee.org/abstract/document/9900151)]

## 2023

<details>
<summary>arXiv</summary>

+ Silent Killer: Optimizing Backdoor Trigger Yields a Stealthy and Powerful Data Poisoning Attack (arXiv 2023) [[code](https://arxiv.org/abs/2301.02615)]
+ Exploring the Limits of Indiscriminate Data Poisoning Attacks (arXiv 2023) [[paper](https://arxiv.org/abs/2303.03592)]
+ Students Parrot Their Teachers: Membership Inference on Model Distillation (arXiv 2023) [[paper](https://arxiv.org/abs/2303.03446)]
+ CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning (arXiv 2023) [[paper](https://arxiv.org/abs/2303.03323)]
+ More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models (arXiv 2023) [[paper](https://arxiv.org/abs/2302.12173)] [[code](https://github.com/greshake/llm-security)]
+ Feature Partition Aggregation: A Fast Certified Defense Against a Union of Sparse Adversarial Attacks (arXiv 2023) [[paper](https://arxiv.org/abs/2302.11628)] [[code](https://github.com/ZaydH/feature-partition)]
+ ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms (arXiv 2023) [[paper](https://arxiv.org/abs/2302.11408)] [[code](https://github.com/ruoxi-jia-group/ASSET)]
+ Temporal Robustness against Data Poisoning (arXiv 2023) [[paper](https://arxiv.org/abs/2302.03684)]
+ A Systematic Evaluation of Backdoor Trigger Characteristics in Image Classification (arXiv 2023) [[paper](http://arxiv.org/abs/2302.01740)]
+ Learning the Unlearnable: Adversarial Augmentations Suppress Unlearnable Example Attacks (arXiv 2023) [[paper](https://arxiv.org/abs/2303.15127)] [[code](https://github.com/lafeat/ueraser)]
+ Backdoor Attacks with Input-unique Triggers in NLP (arXiv 2023) [[paper](https://arxiv.org/abs/2303.14325)]
+ Do Backdoors Assist Membership Inference Attacks? (arXiv 2023) [[paper](https://arxiv.org/abs/2303.12589)]
+ Black-box Backdoor Defense via Zero-shot Image Purification (arXiv 2023) [[paper](https://arxiv.org/abs/2303.12175)]
+ Influencer Backdoor Attack on Semantic Segmentation (arXiv 2023) [[paper](https://arxiv.org/abs/2303.12054)]
+ TrojViT: Trojan Insertion in Vision Transformers (arXiv 2023) [[paper](https://arxiv.org/abs/2208.13049)]
+ Mole Recruitment: Poisoning of Image Classifiers via Selective Batch Sampling (arXiv 2023) [[paper](https://arxiv.org/abs/2303.17080)] [[code](https://github.com/wisdeth14/MoleRecruitment)]
+ Poisoning Web-Scale Training Datasets is Practical (arXiv 2023) [[paper](http://arxiv.org/abs/2302.10149)]
+ Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization (arXiv 2023) [[paper](https://arxiv.org/abs/2304.11823)]
+ MAWSEO: Adversarial Wiki Search Poisoning for Illicit Online Promotion (arXiv 2023) [[paper](https://arxiv.org/abs/2304.11300)]
+ Launching a Robust Backdoor Attack under Capability Constrained Scenarios (arXiv 2023) [[paper](https://arxiv.org/abs/2304.10985)]
+ Certifiable Robustness for Naive Bayes Classifiers (arXiv 2023) [[paper](https://arxiv.org/abs/2303.04811)] [[code](https://github.com/Waterpine/NClean)]
+ Assessing Vulnerabilities of Adversarial Learning Algorithm through Poisoning Attacks (arXiv 2023) [[paper](https://arxiv.org/abs/2305.00399)] [[code](https://github.com/zjfheart/Poison-adv-training)]
+ Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models (arXiv 2023) [[paper](https://arxiv.org/abs/2305.01219)] [[code](https://github.com/shuaizhao95/Prompt_attack)]
+ Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning (arXiv 2023) [[paper](https://arxiv.org/abs/2305.04175)]
+ BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks (arXiv 2023) [[paper](https://arxiv.org/abs/2305.03289)]
+ Backdoor Learning on Sequence to Sequence Models (arXiv 2023) [[paper](https://arxiv.org/abs/2305.02424)]
+ ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger (arXiv 2023) [[paper](https://arxiv.org/abs/2304.14475)]
+ Evil from Within: Machine Learning Backdoors through Hardware Trojans (arXiv 2023) [[paper](https://arxiv.org/abs/2304.08411)]

</details>

+ Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning (ICLR 2023) [[paper](https://openreview.net/forum?id=f0a_dWEYg-Td)]
+ Clean-image Backdoor: Attacking Multi-label Models with Poisoned Labels Only (ICLR 2023) [[paper](https://openreview.net/forum?id=rFQfjDC9Mt)]
+ TrojText: Test-time Invisible Textual Trojan Insertion (ICLR 2023) [[paper](https://openreview.net/forum?id=ja4Lpp5mqc2)] [[code](https://github.com/UCF-ML-Research/TrojText)]
+ Is Adversarial Training Really a Silver Bullet for Mitigating Data Poisoning? (ICLR 2023) [[paper](https://openreview.net/forum?id=zKvm1ETDOq)] [[code](https://github.com/WenRuiUSTC/EntF)]
+ Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning (ICLR 2023) [[paper](https://openreview.net/forum?id=f0a_dWEYg-Td)] [[code](https://github.com/kaiwenzha/contrastive-poisoning)]
+ Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks (ICLR 2023) [[paper](https://openreview.net/forum?id=mkJm5Uy4HrQ)] [[code](https://github.com/charlesjin/compatibility_clustering/)]
+ Revisiting the Assumption of Latent Separability for Backdoor Defenses (ICLR 2023) [[paper](https://openreview.net/forum?id=_wSHsgrVali)] [[code](https://github.com/Unispac/Circumventing-Backdoor-Defenses)]
+ Few-shot Backdoor Attacks via Neural Tangent Kernels (ICLR 2023) [[paper](https://openreview.net/forum?id=a70lGJ-rwy)] [[code](https://github.com/SewoongLab/ntk-backdoor)]
+ SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency (ICLR 2023) [[paper](https://openreview.net/forum?id=o0LFPcoFKnr)] [[code](https://github.com/JunfengGo/SCALE-UP)]
+ Revisiting Graph Adversarial Attack and Defense From a Data Distribution Perspective (ICLR 2023) [[paper](https://openreview.net/forum?id=dSYoPjM5J_W)] [[code](https://github.com/likuanppd/STRG)]
+ Provable Robustness against Wasserstein Distribution Shifts via Input Randomization (ICLR 2023) [[paper](https://openreview.net/forum?id=HJFVrpCaGE)]
+ Donâ€™t forget the nullspace! Nullspace occupancy as a mechanism for out of distribution failure (ICLR 2023) [[paper](https://openreview.net/forum?id=39z0zPZ0AvB)]
+ Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors (ICLR 2023) [[paper](https://openreview.net/forum?id=9MO7bjoAfIA)] [[code](https://github.com/Sizhe-Chen/SEP)]
+ Towards Robustness Certification Against Universal Perturbations (ICLR 2023) [[paper](https://openreview.net/forum?id=7GEvPKxjtt)] [[code](https://github.com/ruoxi-jia-group/Universal_Pert_Cert)]
+ Understanding Influence Functions and Datamodels via Harmonic Analysis (ICLR 2023) [[paper](https://openreview.net/forum?id=cxCEOSF99f)]
+ Distilling Cognitive Backdoor Patterns within an Image (ICLR 2023) [[paper](https://openreview.net/forum?id=S3D9NLzjnQ5)] [[code](https://github.com/HanxunH/CognitiveDistillation)]
+ FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning (ICLR 2023) [[paper](https://openreview.net/forum?id=Xo2E217_M4n)] [[code](https://github.com/KaiyuanZh/FLIP)]
+ UNICORN: A Unified Backdoor Trigger Inversion Framework (ICLR 2023) [[paper](https://openreview.net/forum?id=Mj7K4lglGyj)] [[code](https://github.com/RU-System-Software-and-Security/UNICORN)]
+ Poisoning Language Models During Instruction Tuning (ICML 2023) [[paper](https://arxiv.org/abs/2305.00944)] [[code](https://github.com/AlexWan0/Poisoning-Instruction-Tuned-Models)]
+ Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning (ICML 2023) [[paper](https://arxiv.org/abs/2304.12961)] [[code](https://github.com/ybdai7/Chameleon-durable-backdoor)]
+ Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression (ICML 2023) [[paper](https://arxiv.org/abs/2301.13838)] [[code](https://github.com/liuzrcc/ImageShortcutSqueezing)]
+ Poisoning Generative Replay in Continual Learning to Promote Forgetting (ICML 2023) [[paper](https://openreview.net/forum?id=km7qa1hme2)] [[code](https://www.dropbox.com/sh/mku8oln1t7ngscl/AABVPSwZBlx41GtQYRyYVRgha?dl=0)]
+ Exploring Model Dynamics for Accumulative Poisoning Discovery (ICML 2023) [[paper](https://arxiv.org/abs/2306.03726)] [[code](https://github.com/tmlr-group/Memorization-Discrepancy)]
+ Data Poisoning Attacks Against Multimodal Encoders (ICML 2023) [[paper](https://arxiv.org/abs/2209.15266)] [[code](https://github.com/zqypku/mm_poison/)]
+ Exploring the Limits of Model-Targeted Indiscriminate Data Poisoning Attacks (ICML 2023) [[paper](https://openreview.net/forum?id=r1DAAD9IyE)] [[code](https://github.com/watml/plim)]
+ Run-Off Election: Improved Provable Defense against Data Poisoning Attacks (ICML 2023) [[paper](http://arxiv.org/abs/2302.02300)] [[code](https://github.com/k1rezaei/Run-Off-Election)]
+ Revisiting Data-Free Knowledge Distillation with Poisoned Teachers (ICML 2023) [[paper](https://arxiv.org/abs/2306.02368)] [[code](https://github.com/illidanlab/ABD)]
+ Certified Robust Neural Networks: Generalization and Corruption Resistance (ICML 2023) [[paper](https://arxiv.org/abs/2303.02251)] [[code](https://github.com/RyanLucas3/HR_Neural_Networks)]
+ Understanding Backdoor Attacks through the Adaptability Hypothesis (ICML 2023) [[paper](https://openreview.net/forum?id=iIuLNEnOue)]
+ Robust Collaborative Learning with Linear Gradient Overhead (ICML 2023) [[paper](https://openreview.net/forum?id=BkVWMrgb7K)] [[code](https://github.com/LPD-EPFL/robust-collaborative-learning)]
+ Graph Contrastive Backdoor Attacks (ICML 2023) [[paper](https://openreview.net/forum?id=BfVkbfJGW4)]
+ Reconstructive Neuron Pruning for Backdoor Defense (ICML 2023) [[paper](https://arxiv.org/abs/2305.14876)] [[code](https://github.com/bboylyg/RNP)]
+ Rethinking Backdoor Attacks (ICML 2023) [[paper](https://openreview.net/forum?id=V0ydUD8aW4)]
+ UMD: Unsupervised Model Detection for X2X Backdoor Attacks (ICML 2023) [[paper](https://arxiv.org/abs/2305.18651)]
+ LeadFL: Client Self-Defense against Model Poisoning in Federated Learning (ICML 2023) [[paper](https://openreview.net/forum?id=2CiaH2Tq4G)] [[code](https://github.com/chaoyitud/LeadFL)]
+ RDM-DC: Poisoning Resilient Dataset Condensation with Robust Distribution Matching (UAI 2023) [[paper](https://openreview.net/forum?id=S5KslIBXt_)]
+ Backdoor Defense via Deconfounded Representation Learning (CVPR 2023) [[paper](https://arxiv.org/abs/2303.06818)] [[code](https://github.com/zaixizhang/CBD)]
+ Turning Strengths into Weaknesses: A Certified Robustness Inspired Attack Framework against Graph Neural Networks (CVPR 2023) [[paper](https://arxiv.org/abs/2303.06199)]
+ CUDA: Convolution-based Unlearnable Datasets (CVPR 2023) [[paper](https://arxiv.org/abs/2303.04278)] [[code](https://github.com/vinusankars/Convolution-based-Unlearnability)]
+ Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger (CVPR 2023) [[paper](https://arxiv.org/abs/2302.14677)]
+ Single Image Backdoor Inversion via Robust Smoothed Classifiers (CVPR 2023) [[paper](https://arxiv.org/abs/2303.00215)] [[code](https://github.com/locuslab/smoothinv)]
+ Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples (CVPR 2023) [[paper](https://arxiv.org/abs/2301.01217)] [[code](https://github.com/jiamingzhang94/Unlearnable-Clusters)]
+ Backdoor Defense via Adaptively Splitting Poisoned Dataset (CVPR 2023) [[paper](https://arxiv.org/abs/2303.12993)] [[code](https://github.com/KuofengGao/ASD)]
+ Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency (CVPR 2023) [[paper](https://arxiv.org/abs/2303.18191)] [[code](https://github.com/CGCL-codes/TeCo)]
+ Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning (CVPR 2023) [[paper](https://arxiv.org/abs/2304.01482)] [[code](https://github.com/UCDvision/PatchSearch)]
+ Color Backdoor: A Robust Poisoning Attack in Color Space (CVPR 2023) [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_Color_Backdoor_A_Robust_Poisoning_Attack_in_Color_Space_CVPR_2023_paper.html)]
+ How to Backdoor Diffusion Models? (CVPR 2023) [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Chou_How_to_Backdoor_Diffusion_Models_CVPR_2023_paper.html)] [[code](https://github.com/IBM/BadDiffusion)]
+ Backdoor Cleansing With Unlabeled Data (CVPR 2023) [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Pang_Backdoor_Cleansing_With_Unlabeled_Data_CVPR_2023_paper.html)] [[code](https://github.com/luluppang/BCU)]
+ MEDIC: Remove Model Backdoors via Importance Driven Cloning (CVPR 2023) [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Xu_MEDIC_Remove_Model_Backdoors_via_Importance_Driven_Cloning_CVPR_2023_paper.html)] [[code](https://github.com/qiulingxu/MEDIC)]
+ Architectural Backdoors in Neural Networks (CVPR 2023) [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Bober-Irizar_Architectural_Backdoors_in_Neural_Networks_CVPR_2023_paper.html)]
+ Detecting Backdoors in Pre-Trained Encoders (CVPR 2023) [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Feng_Detecting_Backdoors_in_Pre-Trained_Encoders_CVPR_2023_paper.html)] [[code](https://github.com/GiantSeaweed/DECREE)]
+ The Dark Side of Dynamic Routing Neural Networks: Towards Efficiency Backdoor Injection (CVPR 2023) [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Chen_The_Dark_Side_of_Dynamic_Routing_Neural_Networks_Towards_Efficiency_CVPR_2023_paper.html)] [[code](https://github.com/SeekingDream/CVPR23_EfficFrog)]
+ Progressive Backdoor Erasing via Connecting Backdoor and Adversarial Attacks (CVPR 2023) [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Mu_Progressive_Backdoor_Erasing_via_Connecting_Backdoor_and_Adversarial_Attacks_CVPR_2023_paper.html)]
+ You Are Catching My Attention: Are Vision Transformers Bad Learners Under Backdoor Attacks? (CVPR 2023) [[paper](https://openaccess.thecvf.com/content/CVPR2023/html/Yuan_You_Are_Catching_My_Attention_Are_Vision_Transformers_Bad_Learners_CVPR_2023_paper.html)]
+ Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs (CVPRW 2023) [[paper](https://arxiv.org/abs/2303.13211)]
+ Jigsaw Puzzle: Selective Backdoor Attack to Subvert Malware Classifiers (S&P 2023) [[paper](https://arxiv.org/abs/2202.05470)]
+ SNAP: Efficient Extraction of Private Properties with Poisoning (S&P 2023) [[paper](https://arxiv.org/abs/2208.12348)] [[code](https://github.com/johnmath/snap-sp23)]
+ BayBFed: Bayesian Backdoor Defense for Federated Learning (S&P 2023) [[paper](https://arxiv.org/abs/2301.09508)]
+ RAB: Provable Robustness Against Backdoor Attacks (S&P 2023) [[paper](https://arxiv.org/abs/2003.08904)]
+ FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information (S&P 2023) [[paper](https://arxiv.org/abs/2210.10936)]
+ 3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning [[paper](https://ieeexplore.ieee.org/document/10179401)]
+ Defending against Insertion-based Textual Backdoor Attacks via Attribution (Findings of ACL 2023) [[paper](https://arxiv.org/abs/2305.02394)]
+ Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias (Findings of ACL 2023) [[paper](https://arxiv.org/abs/2305.04547)]
+ Defending Against Backdoor Attacks by Layer-wise Feature Analysis (PAKDD 2023) [[paper](https://arxiv.org/abs/2302.12758)] [[code](https://github.com/NajeebJebreel/DBALFA)]
+ Manipulating Federated Recommender Systems: Poisoning with Synthetic Users and Its Countermeasures (SIGIR 2023) [[paper](https://arxiv.org/abs/2304.03054)]
+ The Dark Side of Explanations: Poisoning Recommender Systems with Counterfactual Examples (SIGIR 2023) [[paper](https://arxiv.org/abs/2305.00574)]
+ How to Sift Out a Clean Data Subset in the Presence of Data Poisoning? (USENIX Security 2023) [[paper](http://arxiv.org/abs/2210.06516)] [[code](https://github.com/ruoxi-jia-group/Meta-Sift)]
+ PORE: Provably Robust Recommender Systems against Data Poisoning Attacks (USENIX Security 2023) [[paper](https://arxiv.org/abs/2303.14601)]
+ On the Security Risks of Knowledge Graph Reasoning (USENIX Security 2023) [[paper](https://arxiv.org/abs/2305.02383)] [[code](https://github.com/HarrialX/security-risk-KG-reasoning)]
+ BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT (NDSS 2023) [[paper](https://arxiv.org/abs/2304.12298)]
+ Exploiting Logic Locking for a Neural Trojan Attack on Machine Learning Accelerators (GLSVLSI 2023) [[paper](https://arxiv.org/abs/2304.06017)]
+ Energy-Latency Attacks to On-Device Neural Networks via Sponge Poisoning (SecTL 2023) [[paper](https://arxiv.org/abs/2305.03888)]
+ Beyond the Model: Data Pre-processing Attack to Deep Learning Models in Android Apps (SecTL 2023) [[paper](https://arxiv.org/abs/2305.03963)]

## 2022

+ Transferable Unlearnable Examples (arXiv 2022) [[paper](https://arxiv.org/abs/2210.10114)]
+ Natural Backdoor Datasets (arXiv 2022) [[paper](http://arxiv.org/abs/2206.10673)]
+ Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World (arXiv 2022) [[paper](https://arxiv.org/abs/2201.08619)]
+ Backdoor Attacks on Self-Supervised Learning (CVPR 2022) [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Saha_Backdoor_Attacks_on_Self-Supervised_Learning_CVPR_2022_paper.html)] [[code](https://github.com/UMBCvision/SSL-Backdoor)]
+ Poisons that are learned faster are more effective (CVPR 2022 Workshops) [[paper](http://arxiv.org/abs/2204.08615)]
+ Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning (ICLR 2022) [[paper](https://openreview.net/forum?id=baUQQPwQiAg)] [[code](https://github.com/fshp971/robust-unlearnable-examples)]
+ Adversarial Unlearning of Backdoors via Implicit Hypergradient (ICLR 2022) [[paper](https://openreview.net/forum?id=MeeQkFYVbzW)] [[code](https://github.com/YiZeng623/I-BAU)]
+ Not All Poisons are Created Equal: Robust Training against Data Poisoning (ICML 2022) [[paper](https://proceedings.mlr.press/v162/yang22j.html)] [[code](https://github.com/YuYang0901/EPIC)]
+ Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch (NeurIPS 2022) [[paper](http://arxiv.org/abs/2106.08970)] [[code](https://github.com/hsouri/Sleeper-Agent)]
+ Hidden Poison: Machine unlearning enables camouflaged poisoning attacks (NeurIPS 2022 Workshop MLSW) [[paper](https://openreview.net/forum?id=zml9gDnulI9)]
+ Policy Resilience to Environment Poisoning Attacks
  on Reinforcement Learning (NeurIPS 2022 Workshop MLSW) [[paper](https://arxiv.org/abs/2304.12151)]
+ Hard to Forget: Poisoning Attacks on Certified Machine Unlearning (AAAI 2022) [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/20736)] [[code](https://github.com/ngmarchant/attack-unlearning)]
+ Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks (AAAI 2022) [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/21191)]
+ PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning (USENIX Security 2022) [[paper](http://arxiv.org/abs/2205.06401)]
+ Planting Undetectable Backdoors in Machine Learning Models (FOCS 2022) [[paper](https://ieeexplore.ieee.org/abstract/document/9996741)]

## 2021

+ DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations (arXiv 2021) [[paper](https://arxiv.org/abs/2103.02079)]
+ How Robust Are Randomized Smoothing Based Defenses to Data Poisoning? (CVPR 2021) [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Mehra_How_Robust_Are_Randomized_Smoothing_Based_Defenses_to_Data_Poisoning_CVPR_2021_paper.html)]
+ Preventing Unauthorized Use of Proprietary Data: Poisoning for Secure Dataset Release (ICLR 2021 Workshop on Security and Safety in Machine Learning Systems) [[paper](http://arxiv.org/abs/2103.02683)]
+ Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching (ICLR 2021) [[paper](https://arxiv.org/abs/2009.02276)] [[code](https://github.com/JonasGeiping/poisoning-gradient-matching)]
+ Unlearnable Examples: Making Personal Data Unexploitable (ICLR 2021) [[paper](https://arxiv.org/abs/2101.04898)] [[code](https://github.com/HanxunH/Unlearnable-Examples)]
+ Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks (ICLR 2021) [[paper](http://arxiv.org/abs/2101.05930)] [[code](https://github.com/akshaymehra24/PoisoningCertifiedDefenses)]
+ LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition (ICLR 2021) [[paper](https://openreview.net/forum?id=hJmtwocEqzc)]
+ What Doesn't Kill You Makes You Robust(er): How to Adversarially Train against Data Poisoning (ICLR 2021 Workshop) [[paper](https://arxiv.org/abs/2102.13624)]
+ Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks (ICML 2021) [[paper](https://arxiv.org/pdf/2006.12557)] [[code](https://github.com/aks2203/poisoning-benchmark)]
+ Neural Tangent Generalization Attacks (ICML 2021) [[paper](https://proceedings.mlr.press/v139/yuan21b.html)]
+ SPECTRE: Defending Against Backdoor Attacks Using Robust Covariance Estimation (ICML 2021) [[paper](https://proceedings.mlr.press/v139/hayase21a.html)]
+ Adversarial Examples Make Strong Poisons (NeurIPS 2021) [[paper](https://proceedings.neurips.cc/paper/2021/hash/fe87435d12ef7642af67d9bc82a8b3cd-Abstract.html)]
+ Anti-Backdoor Learning: Training Clean Models on Poisoned Data (NeurIPS 2021) [[paper](https://proceedings.neurips.cc/paper/2021/hash/7d38b1e9bd793d3f45e0e212a729a93c-Abstract.html)] [[code](https://github.com/bboylyg/ABL)]
+ Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective (ICCV 2021) [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Zeng_Rethinking_the_Backdoor_Attacks_Triggers_A_Frequency_Perspective_ICCV_2021_paper.html)] [[code](https://github.com/YiZeng623/frequency-backdoor)]
+ Intrinsic Certified Robustness of Bagging against Data Poisoning Attacks (AAAI 2021) [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/16971)] [[code](https://github.com/jinyuan-jia/BaggingCertifyDataPoisoning)]
+ Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff (ICASSP 2021) [[paper](https://ieeexplore.ieee.org/abstract/document/9414862)]

## 2020

+ On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping (arXiv 2020) [[paper](https://arxiv.org/abs/2002.11497)] [[code](https://github.com/Sanghyun-Hong/Gradient-Shaping)]
+ Backdooring and poisoning neural networks with image-scaling attacks (arXiv 2020) [[paper](https://arxiv.org/abs/2003.08633)]
+ Poisoned classifiers are not only backdoored, they are fundamentally broken (arXiv 2020) [[paper](https://arxiv.org/abs/2010.09080)] [[code](https://github.com/locuslab/breaking-poisoned-classifier)]
+ Invisible backdoor attacks on deep neural networks via steganography and regularization (TDSC 2020) [[paper](https://ieeexplore.ieee.org/abstract/document/9186317)]
+ Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs (CVPR 2020) [[paper](https://arxiv.org/abs/1906.10842)] [[code](https://github.com/UMBCvision/Universal-Litmus-Patterns)]
+ MetaPoison: Practical General-purpose Clean-label Data Poisoning (NeurIPS 2020) [[paper](https://arxiv.org/abs/2004.00225)]
+ Input-Aware Dynamic Backdoor Attack (NeurIPS 2020) [[paper](https://proceedings.neurips.cc/paper/2020/hash/234e691320c0ad5b45ee3c96d0d7b8f8-Abstract.html)] [[code](https://github.com/VinAIResearch/input-aware-backdoor-attack-release)]
+ How To Backdoor Federated Learning (AISTATS 2020) [[paper](https://proceedings.mlr.press/v108/bagdasaryan20a.html)]
+ Reflection backdoor: A natural backdoor attack on deep neural networks (ECCV 2020) [[paper](https://arxiv.org/abs/2007.02343)]
+ Practical Poisoning Attacks on Neural Networks (ECCV 2020) [[paper](https://link.springer.com/chapter/10.1007/978-3-030-58583-9_9)]
+ Practical Detection of Trojan Neural Networks: Data-Limited and Data-Free Cases (ECCV 2020) [[paper](https://link.springer.com/chapter/10.1007/978-3-030-58592-1_14)] [[code](https://github.com/wangren09/TrojanNetDetector)]
+ Deep k-NN Defense Against Clean-Label Data Poisoning Attacks (ECCV 2020 Workshops) [[paper](https://link.springer.com/chapter/10.1007/978-3-030-66415-2_4)] [[code](https://github.com/neeharperi/DeepKNNDefense)]
+ Radioactive data: tracing through training (ICML 2020) [[paper](https://arxiv.org/abs/2002.00937)]
+ Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks (ICML 2020) [[paper](https://proceedings.mlr.press/v119/croce20b.html)]
+ Certified Robustness to Label-Flipping Attacks via Randomized Smoothing (ICML 2020) [[paper](http://proceedings.mlr.press/v119/rosenfeld20b.html)]
+ An Embarrassingly Simple Approach for Trojan Attack in Deep Neural Networks (KDD 2020) [[paper](https://dl.acm.org/doi/abs/10.1145/3394486.3403064)] [[code](https://github.com/trx14/TrojanNet)]
+ Hidden Trigger Backdoor Attacks (AAAI 2020) [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/6871)] [[code](https://github.com/UMBCvision/Hidden-Trigger-Backdoor-Attacks)]

## 2019

+ Label-consistent backdoor attacks (arXiv 2019) [[paper](https://arxiv.org/abs/1912.02771)]
+ Poisoning Attacks with Generative Adversarial Nets (arXiv 2019) [[paper](https://arxiv.org/abs/1906.07773)]
+ TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems (arXiv 2019) [[paper](https://arxiv.org/abs/1908.01763)]
+ BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain (IEEE Access 2019) [[paper](https://arxiv.org/abs/1708.06733)]
+ Data Poisoning against Differentially-Private Learners: Attacks and Defenses (IJCAI 2019) [[paper](https://arxiv.org/abs/1903.09860)]
+ DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks (IJCAI 2019) [[paper](https://aceslab.org/sites/default/files/DeepInspect.pdf)]
+ Sever: A Robust Meta-Algorithm for Stochastic Optimization (ICML 2019) [[paper](https://proceedings.mlr.press/v97/diakonikolas19a.html)]
+ Learning with Bad Training Data via Iterative Trimmed Loss Minimization (ICML 2019) [[paper](https://proceedings.mlr.press/v97/shen19e.html)]
+ Universal Multi-Party Poisoning Attacks (ICML 2019) [[paper](https://proceedings.mlr.press/v97/mahloujifar19a.html)]
+ Transferable Clean-Label Poisoning Attacks on Deep Neural Nets (ICML 2019) [[paper](https://arxiv.org/abs/1905.05897)]
+ Defending Neural Backdoors via Generative Distribution Modeling (NeurIPS 2019) [[paper](https://arxiv.org/abs/1910.04749)]
+ Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder (NeurIPS 2019) [[paper](https://proceedings.neurips.cc/paper/2019/hash/1ce83e5d4135b07c0b82afffbe2b3436-Abstract.html)]
+ The Curse of Concentration in Robust Learning: Evasion and Poisoning Attacks from Concentration of Measure (AAAI 2019) [[paper](https://arxiv.org/abs/1809.03063)]
+ Backdoor Attacks against Transfer Learning with Pre-trained Deep Learning Models (IEEE Transactions on Services Computing 2019) [[paper](https://arxiv.org/abs/2001.03274)]
+ Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks (IEEE Symposium on Security and Privacy 2019) [[paper](https://ieeexplore.ieee.org/abstract/document/8835365)]
+ STRIP: a defence against trojan attacks on deep neural networks (ACSAC 2019) [[paper](https://dl.acm.org/doi/abs/10.1145/3359789.3359790?casa_token=JDIdKo9xPV8AAAAA:SPvqGbP3MHjAr6wkOZ6jc7ZEmh_64APDkQ9aDO2WmCyaWL83dU7vHR5-Rjg-aNVDBo0_QU3TOn7uvQ)]

## 2018

+ Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering (arXiv 2018) [[paper](https://arxiv.org/abs/1811.03728)]
+ Spectral Signatures in Backdoor Attacks (NeurIPS 2018) [[paper](https://proceedings.neurips.cc/paper/2018/hash/280cf18baf4311c92aa5a042336587d3-Abstract.html)]
+ Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks (NeurIPS 2018) [[paper](https://arxiv.org/abs/1804.00792)]
+ Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise (NeurIPS 2018) [[paper](https://arxiv.org/abs/1802.05300)]
+ Trojaning Attack on Neural Networks (NDSS 2018) [[paper](https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_03A-5_Liu_paper.pdf)]
+ Label Sanitization Against Label Flipping Poisoning Attacks (ECML PKDD 2018 Workshops) [[paper](https://link.springer.com/chapter/10.1007/978-3-030-13453-2_1)]
+ Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring (USENIX Security 2018) [[paper](https://arxiv.org/abs/1802.04633)]

## 2017

+ Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning (arXiv 2017) [[paper](https://arxiv.org/abs/1712.05526)]
+ Generative Poisoning Attack Method Against Neural Networks (arXiv 2017) [[paper](https://arxiv.org/abs/1703.01340)]
+ Delving into Transferable Adversarial Examples and Black-box Attacks (ICLR 2017) [[paper](https://openreview.net/forum?id=Sys6GJqxl)]
+ Understanding Black-box Predictions via Influence Functions (ICML 2017) [[paper](http://proceedings.mlr.press/v70/koh17a?ref=https://githubhelp.com)] [[code](https://github.com/kohpangwei/influence-release)]
+ Certified Defenses for Data Poisoning Attacks (NeurIPS 2017) [[paper](https://arxiv.org/abs/1706.03691)]

## 2016

+ Data Poisoning Attacks on Factorization-Based Collaborative Filtering (NeurIPS 2016) [[paper](https://proceedings.neurips.cc/paper/2016/hash/83fa5a432ae55c253d0e60dbfa716723-Abstract.html)]

## 2015

+ Is Feature Selection Secure against Training Data Poisoning? (ICML 2015) [[paper](https://proceedings.mlr.press/v37/xiao15.html)]
+ Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners (AAAI 2015) [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/9569)]
